---
title: "Career Choice and Academic Performance"
author: "Chris Cioffi, Kristina Frazier, Aidan Hennessy, Mike McHenry"
format:
  revealjs:
    theme: "night"
embed-resources: true
---

## Overview

1)  Introduction and Question
2)  Background and Related Literature
3)  Data
4)  Exploratory Analysis
5)  Modeling
6)  Diagnostic Tools
7)  Remedial Measures
8)  Conclusion

## What Do You Want to Be When You Grow Up?

-   During high school, young adults are often asked to make decisions regarding post-secondary education that can have a profound and lasting impact on their lives in the future.

-   We investigate what factors in high school may be related to future academic performance.

## Research Question

-   Question: How is college GPA related to prospective career path in high school? How are other characteristics about a student's background and high school environment related to their college GPA?

-   This study aims to investigate whether students who have a desired future career path in the 9th grade perform better than students who do not, and if choice of career path matters.

## Data

-   High School Longitudinal Study of 2009 (HSLS:09) from the National Center for Education Statistics.
    -   Interviewed 9th graders across the United States in 2009.
    -   Followed up with subjects in three subsequent interview rounds.
    -   Offers a variety of information on students, parents, and school.

## Key Variables

-   Response Variable: College GPA

-   Primary Predictor of Interest: Desired occupation at age 30.

    -   A categorical variable with 22 occupation groups.

-   Additional predictors:

    -   Academic: High school GPA, credits earned for AP/IB courses, School engagement, Stem/non-stem desired occupation
    -   Geographic and Socioeconomic Factors: Family Income, High School urbanicity, High School type

## A Look at Desired Occupation

```{r}
#| include: false
library(tidyverse)
library(broom)
library(patchwork)
library(lmtest)
load("../Data/HSLS_Data.rdata")
```

```{r}
#| echo: false

# X5GPAALL: Postsecondary Transcript: GPA at all known institutions attended
# X1STU30OCC2: Planned student occupation at age 30

HSLS_Data |>
  filter(
    X5GPAALL >= 0,
    X1STU30OCC2 != "Unit non-response"
  ) |>
  filter(X1STU30OCC2 != "Uncodeable") |>
  filter(X1STU30OCC2 != "Missing") |>
  select(X5GPAALL, X1STU30OCC2) |>
  ggplot(aes(x = X5GPAALL, y = X1STU30OCC2)) +
  geom_boxplot() +
  labs(
    title = "College GPA by Desired Occupation",
    x = "College GPA",
    y = "Desired Occupation"
  ) +
  theme_bw()

# NOTE: Try to sort this by Median GPA
```

## Desired Occupation and Academic Performance
*Color-coded by Planned student occupation at age 30*

```{r}
#| echo: false

# X3TGPAACAD: GPA for all academic courses

HSLS_Data |>
  filter(
    X5GPAALL >= 0,
    X3TGPAACAD >= 0,
    X1STU30OCC2 != "Unit non-response"
  ) |>
  filter(X1STU30OCC2 != "Uncodeable") |>
  filter(X1STU30OCC2 != "Missing") |>
  select(X5GPAALL, X3TGPAACAD, X1STU30OCC2) |>
  ggplot(aes(x = X3TGPAACAD, y = X5GPAALL, color = X1STU30OCC2)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  theme(legend.position = "none") +
  labs(
    title = "HS GPA vs. College GPA by Desired Occupation",
    x = "High School GPA",
    y = "College GPA"
  ) +
  theme_bw() +
  theme(legend.position = "none")
```

```{r}
#| echo: false
HSLS_Data |>
  filter(
    X5GPAALL >= 0,
    X3TGPAACAD >= 0,
    X1STU30OCC2 != "Unit non-response"
  ) |>
  filter(X1STU30OCC2 != "Uncodeable") |>
  filter(X1STU30OCC2 != "Missing") |>
  rename(College_GPA = X5GPAALL, HS_GPA = X3TGPAACAD) |>
  select(College_GPA, HS_GPA) |>
  cor()
```



## Model: Simple Linear Regression

-   Set reference group to those students who answered "Don't Know".

-   Model takes the form of $College\_GPA = \beta_0 + \beta_1future\_job + \epsilon$.

## Results: Simple Linear Regression

-   Showing only results with a p-value \< 0.10.

```{r}
#| echo: false
# Build df
SLR_GPA_Job <- HSLS_Data |>
  filter(
    X5GPAALL >= 0,
    X1STU30OCC2 != "Unit non-response"
  ) |>
  filter(X1STU30OCC2 != "Uncodeable") |>
  filter(X1STU30OCC2 != "Missing") |>
  select(X5GPAALL, X1STU30OCC2)

# Set reference group to "Don't know"
SLR_GPA_Job$X1STU30OCC2 <- relevel(SLR_GPA_Job$X1STU30OCC2, ref = "Don't know")

# Regression
SLR_GPA_Job_reg <- lm(X5GPAALL ~ X1STU30OCC2, data = SLR_GPA_Job)
summary(SLR_GPA_Job_reg) |>
  broom::tidy() |>
  filter(p.value <= 0.01) |>
  mutate(term = str_remove(term, "X1STU30OCC2"))

# For the sake of reducing the output, just show results that are significant at alpha = 0.10.
```

-   Adjusted $R^2$ is less than 1%.

## Additional Variables


```{r}
HSLS_Data |>
  filter(
    X5GPAALL >= 0,
    X4ENTRYMAJ23 != "Missing",
    X4ENTRYMAJ23 != "Unit non-response",
    X4ENTRYMAJ23 != "Item legitimate skip/NA"
  ) |>
  select(X4ENTRYMAJ23, X5GPAALL) |>
  ggplot(aes(x = X4ENTRYMAJ23, y = X5GPAALL)) +
  geom_boxplot() +
  coord_flip() +
  labs(
    title = "College GPA by College Major Considering",
    x = "College GPA",
    y = "College Major Considering"
  ) +
  theme_bw() -> major

major
```


## Additional Variables

```{r}
HSLS_Data |>
  filter(
    X5GPAALL >= 0,
    X1FAMINCOME != "Item legitimate skip/NA",
    X1FAMINCOME != "Unit non-response",
    X1FAMINCOME != "Missing"
  ) |>
  select(X5GPAALL, X1FAMINCOME) |>
  #mutate(X1FAMINCOME = factor(X1FAMINCOME, levels = unique(X1FAMINCOME))) |>
  ggplot(aes(x = X5GPAALL, y = reorder(X1FAMINCOME, X1FAMINCOME))) +
  geom_boxplot() +
  labs(title = "College GPA by Family Income",
       x = "College GPA", 
       y = "Total family income from all sources 2008") +
  theme_bw() -> income

income
```


## Additional Variables

```{r}
HSLS_Data |>
  filter(
    X5GPAALL >= 0,
    X1LOCALE != "Item legitimate skip/NA",
    X1LOCALE != "Unit non-response",
    X1LOCALE != "Missing"
  ) |>
  select(X5GPAALL, X1LOCALE) |>
  mutate(X1LOCALE = factor(X1LOCALE, levels = unique(X1LOCALE))) |>
  ggplot(aes(x = X5GPAALL, y = X1LOCALE)) +
  geom_boxplot() +
  coord_flip() +
  labs(title = "College GPA by Urbanicity",
       x = "College GPA", 
       y = "Urbanicity") +
  theme_bw() -> urbanicity

HSLS_Data |>
  filter(
    X5GPAALL >= 0,
    X3TCREDAPIB >= 0,
    X1LOCALE != "Item legitimate skip/NA",
    X1LOCALE != "Unit non-response",
    X1LOCALE != "Missing"
  ) |>
  select(X5GPAALL, X3TCREDAPIB) |>
  ggplot(aes(x = X3TCREDAPIB, y = X5GPAALL)) +
  geom_point() +
  labs(title = "College GPA by AP/IB Credits",
       x = "College GPA", 
       y = "AP/IB Credits") +
  theme_bw() -> credits

urbanicity /
credits
```

## Additional Variables

```{r}
HSLS_Data |>
  filter( X5GPAALL >= 0
  ) |>
  select(X5GPAALL, X1CONTROL) |>
  mutate(X1CONTROL = factor(X1CONTROL, levels = unique(X1CONTROL))) |>
  ggplot(aes(x = X5GPAALL, y = X1CONTROL)) +
  geom_boxplot() +
  coord_flip() +
  labs(title = "College GPA by School Type",
       x = "College GPA", 
       y = " School Type") +
  theme_bw() -> type

HSLS_Data |>
  filter(X5GPAALL >= 0) |>
  select(X5GPAALL, X1SCHOOLBEL) |>
  ggplot(aes(x = X5GPAALL, y = X1SCHOOLBEL)) +
  geom_point() +
  labs(title = "College GPA by Studentâ€™s Sense of School Belonging",
       x = "College GPA", 
       y = "School Belonging") +
  theme_bw() -> belonging

type /
belonging
```



## Multiple Linear Regression

```{r}
#| echo: false

# Remove missing values
MLR_all <- HSLS_Data |>
  filter(
    X5GPAALL >= 0,
    X3TGPAACAD >= 0,
    X3TCREDAPIB >= 0,
    X1STU30OCC2 != "Unit non-response",
    X1STU30OCC2 != "Uncodeable",
    X1STU30OCC2 != "Missing",
    X4ENTRYMAJ23 != "Missing",
    X4ENTRYMAJ23 != "Unit non-response",
    X4ENTRYMAJ23 != "Item legitimate skip/NA",
    X1LOCALE != "Item legitimate skip/NA",
    X1LOCALE != "Unit non-response",
    X1LOCALE != "Missing",
    X1FAMINCOME != "Item legitimate skip/NA",
    X1FAMINCOME != "Unit non-response",
    X1FAMINCOME != "Missing"
  )


# Reset reference groups
MLR_all$X1STU30OCC2 <- relevel(MLR_all$X1STU30OCC2, ref = "Don't know")
MLR_all$X4ENTRYMAJ23 <- relevel(MLR_all$X4ENTRYMAJ23, ref = "Undeclared/undecided")
MLR_all$X1LOCALE <- relevel(MLR_all$X1LOCALE, ref = "Rural")
```

```{r}
#| echo: false
# unique(MLR_all$X5GPAALL)
# unique(MLR_all$X1STU30OCC2)
# unique(MLR_all$X3TGPAACAD)
# unique(MLR_all$X4ENTRYMAJ23)
# unique(MLR_all$X1LOCALE)
# unique(MLR_all$X1FAMINCOME)
# unique(MLR_all$X3TCREDAPIB)
# unique(MLR_all$X1CONTROL)
# unique(MLR_all$X1SCHOOLBEL)
```


Initial MLR Model: 
$$College\_GPA = \\
\beta_0 + \beta_1future\_job + \beta_2college\_gpa + \beta_3major\_considering + \\ \beta_4family\_income + \beta_5credits + \beta_6school\_type + \beta_7urbanicity + \\ \beta_8school\_belonging + \epsilon$$

- Adjusted $R^2$: 0.3563
- F-statistic: 66.59 on 62 and 7286 DF,  p-value: < 2.2e-16

```{r}
#| echo: false
MLR_all.reg <- lm(X5GPAALL ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1LOCALE + X1FAMINCOME + X3TCREDAPIB + X1CONTROL + X1SCHOOLBEL, data = MLR_all)
```



```{r}
#| include: false
summary(MLR_all.reg)
  # broom::tidy() |>
  # select(term, p.value) |>
  # mutate(p.value = sprintf("%.2f", p.value))
# filter(p.value >= 0.01)
```



## Remove Urbanicity & School Belonging?

- Urbanicity & School Belonging: All Betas are insignificant at alpha = 0.10

**Lack of Fit Test**

- Null Hypothesis: $\beta_7urbanicity$ = $\beta_8school\_belonging$ = 0

- Alternative hypothesis: either of the betas for these variables is a non-zero value

- Use alpha = 0.10

## Remove Urbanicity & School Belonging?

```{r}
#| echo: false
MLR_all.reg.reduced <- lm(X5GPAALL ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, data = MLR_all)
anova(MLR_all.reg.reduced, MLR_all.reg)
```

- With a P-value of 0.331, there is insufficient evidence to reject the null hypothesis that the values for the betas of these two predictors are not zero.

- The lack of significant relationship between Urbanicity & School Belonging was seen in earlier plots.

## Variable Selection

New MLR Model: 
$$College\_GPA = \\
\beta_0 + \beta_1future\_job + \beta_2college\_gpa + \beta_3major\_considering + \\ \beta_4family\_income + \beta_5credits + \beta_6school\_type + \epsilon$$


```{r}
#| include: false
step(MLR_all.reg.reduced,
  direction = "both",
  scope = list(lower = ~X5GPAALL ~ X1STU30OCC2)
) # force inclusion of Planned occupation (X1STU30OCC2)
```

- Stepwise selection did not remove additional variables

## Diagnostics

*Linearity*

- F-statistic:  71.1 on 58 and 7290 DF,  p-value: < 2.2e-16

```{r}
#| echo: false
summary(MLR_all.reg.reduced)
```


## Diagnostics

*Constant Variance*

- Breusch-Pagan test yields a p-value < 0.0001.

```{r}
#| include: false
library(lmtest)
bptest(MLR_all.reg.reduced)
```


- Because the response variable is bound.

```{r}
residuals <- residuals(MLR_all.reg.reduced)
predicted <- predict(MLR_all.reg.reduced)

# Plot residuals vs predicted values
plot(predicted, residuals,
  pch = 19, col = "blue", xlab = "Predicted Values (Y-Hat)", ylab = "Residuals",
  main = "Residuals vs Predicted Values"
)
abline(h = 0, col = "red") # Add a horizontal line at y = 0 for reference
```



```{r, fig.show='hide'}

predictor_vars_set1 <- c("X1STU30OCC2", "X3TGPAACAD", "X4ENTRYMAJ23")

lm_list_set1 <- lapply(predictor_vars_set1, function(var) lm(formula = paste("X5GPAALL ~", var), data = MLR_all))

par(mfrow=c(2, 2)) 
for (i in seq_along(predictor_vars_set1)) {
  residuals_i <- residuals(lm_list_set1[[i]])
  predicted_i <- predict(lm_list_set1[[i]])
  plot(predicted_i, residuals_i,
       pch = 19, col = adjustcolor("blue", alpha.f = 0.5), xlab = "Predicted Values (Y-Hat)", ylab = "Residuals",
       main = paste(predictor_vars_set1[i], ": Residuals vs Predicted Values"))
  abline(h = 0, col = "red") 
}


```




```{r, fig.show='hide'}

predictor_vars_set2 <- c("X1FAMINCOME", "X3TCREDAPIB", "X1CONTROL")


lm_list_set2 <- lapply(predictor_vars_set2, function(var) lm(formula = paste("X5GPAALL ~", var), data = MLR_all))


par(mfrow=c(2, 2)) 
for (i in seq_along(predictor_vars_set2)) {
  residuals_i <- residuals(lm_list_set2[[i]])
  predicted_i <- predict(lm_list_set2[[i]])
  plot(predicted_i, residuals_i,
       pch = 19, col = adjustcolor("blue", alpha.f = 0.5), xlab = "Predicted Values (Y-Hat)", ylab = "Residuals",
       main = paste(predictor_vars_set2[i], ": Residuals vs Predicted Values"))
  abline(h = 0, col = "red") # Add a horizontal line at y = 0 for reference
}

```



## Diagnostics

*Normality*

```{r}
par(mfrow = c(1, 2))
hist(MLR_all.reg.reduced$resi)
qqnorm(MLR_all.reg.reduced$resi)
qqline(MLR_all.reg.reduced$resi)
```


## Remedial Measures

-   Need to address non-constant variance first, and then recheck normality assumption

-   Try Box-Cox transformation

-   Weighted Least Squares

-   Recheck model diagnostics.


## Try Box-Cox Transformation

Need to get rid of all 0.0 GPAs

```{r}
# Get rid of 0.0 GPA
MLR_all_no_0 <- MLR_all |>
  filter(X5GPAALL > 0)

MLR_all.reg.reduced.logy <- lm(log(X5GPAALL) ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, data = MLR_all_no_0)

MLR_all.reg.reduced.logy
```

## Box-Cox Residual Variance

Still a pattern

```{r}
residuals <- residuals(MLR_all.reg.reduced.logy)
predicted <- predict(MLR_all.reg.reduced.logy)

# Plot residuals vs predicted values
plot(predicted, residuals,
  pch = 19, col = "blue", xlab = "Predicted Values (Y-Hat)", ylab = "Residuals",
  main = "Residuals vs Predicted Values"
)
abline(h = 0, col = "red") # Add a horizontal line at y = 0 for reference
```

## Weighted Least Squares on Full Model

Can't reach coefficient convergence.

```{r}
# Obtain variance function for WLS
MLR_all_WLS <- MLR_all
MLR_all_WLS$residuals <- residuals(MLR_all.reg.reduced)
var_func <- lm(residuals^2 ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, data = MLR_all_WLS)

# Obtain weights
MLR_all_WLS <- MLR_all_WLS |>
mutate(w1 = 1/abs(var_func$fitted))

# First WLS Model
MLR_all.reg.reduced.WLS <- lm(X5GPAALL ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, weights = w1, data = MLR_all_WLS)

# Iterate
MLR_all_WLS$residuals_wls1 <- residuals(MLR_all.reg.reduced.WLS)
var_func_2 <- lm(residuals_wls1^2 ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, data = MLR_all_WLS)

# Obtain weights
MLR_all_WLS <- MLR_all_WLS |>
mutate(w2 = 1/abs(var_func_2$fitted))

# Second WLS Model
MLR_all.reg.reduced.WLS2 <- lm(X5GPAALL ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, weights = w2, data = MLR_all_WLS)

# Iterate 2
MLR_all_WLS$residuals_wls2 <- residuals(MLR_all.reg.reduced.WLS2)
var_func_3 <- lm(residuals_wls2^2 ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, data = MLR_all_WLS)

# Obtain weights
MLR_all_WLS <- MLR_all_WLS |>
mutate(w3 = 1/abs(var_func_3$fitted))

# Third WLS Model
MLR_all.reg.reduced.WLS3 <- lm(X5GPAALL ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, weights = w3, data = MLR_all_WLS)

# Iterate 3
MLR_all_WLS$residuals_wls3 <- residuals(MLR_all.reg.reduced.WLS3)
var_func_4 <- lm(residuals_wls3^2 ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, data = MLR_all_WLS)

# Obtain weights
MLR_all_WLS <- MLR_all_WLS |>
mutate(w4 = 1/abs(var_func_4$fitted))

# Fourth WLS Model
MLR_all.reg.reduced.WLS4 <- lm(X5GPAALL ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, weights = w4, data = MLR_all_WLS)

# Iterate 4
MLR_all_WLS$residuals_wls4 <- residuals(MLR_all.reg.reduced.WLS4)
var_func_5 <- lm(residuals_wls4^2 ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, data = MLR_all_WLS)

# Obtain weights
MLR_all_WLS <- MLR_all_WLS |>
mutate(w5 = 1/abs(var_func_5$fitted))

# Fifth WLS Model
MLR_all.reg.reduced.WLS5 <- lm(X5GPAALL ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, weights = w5, data = MLR_all_WLS)

# Iterate 5
MLR_all_WLS$residuals_wls5 <- residuals(MLR_all.reg.reduced.WLS5)
var_func_6 <- lm(residuals_wls5^2 ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, data = MLR_all_WLS)

# Obtain weights
MLR_all_WLS <- MLR_all_WLS |>
mutate(w6 = 1/abs(var_func_6$fitted))

# Sixth WLS Model
MLR_all.reg.reduced.WLS6 <- lm(X5GPAALL ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, weights = w6, data = MLR_all_WLS)

# Iterate 6
MLR_all_WLS$residuals_wls6 <- residuals(MLR_all.reg.reduced.WLS6)
var_func_7 <- lm(residuals_wls6^2 ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, data = MLR_all_WLS)

# Obtain weights
MLR_all_WLS <- MLR_all_WLS |>
mutate(w7 = 1/abs(var_func_7$fitted))

# Seventh WLS Model
MLR_all.reg.reduced.WLS7 <- lm(X5GPAALL ~ X1STU30OCC2 + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, weights = w7, data = MLR_all_WLS)

# Plot residual variance
residuals <- rstandard(MLR_all.reg.reduced.WLS7)
predicted <- predict(MLR_all.reg.reduced.WLS7)

# Plot residuals vs predicted values
plot(predicted, residuals,
  pch = 19, col = "blue", xlab = "Predicted Values (Y-Hat)", ylab = "Residuals",
  main = "Standardized Residuals vs Predicted Values"
)
abline(h = 0, col = "red") # Add a horizontal line at y = 0 for reference

```

## Make Predictor "Don't Know" vs. "Know" Future Occupation

```{r}
MLR_all <- MLR_all |>
  mutate(career = ifelse(X1STU30OCC2 == "Don't know", 0, 1), .after = X1STU30OCC2)

#reg_career <- lm(X5GPAALL ~ career + X3TGPAACAD + X4ENTRYMAJ23 + X1FAMINCOME + X3TCREDAPIB + X1CONTROL, data = MLR_all)
reg_career <- lm(X5GPAALL ~ career + X3TGPAACAD, data = MLR_all)

summary(reg_career)
```

## 0-1 Career Residual Variance

Still a pattern.

```{r}
residuals <- rstandard(reg_career)
predicted <- predict(reg_career)

# Plot residuals vs predicted values
plot(predicted, residuals,
  pch = 19, col = "blue", xlab = "Predicted Values (Y-Hat)", ylab = "Residuals",
  main = "Residuals vs Predicted Values"
)
abline(h = 0, col = "red") # Add a horizontal line at y = 0 for reference
```

```{r}
# Try WLS for career dummy variable

# Obtain variance function for WLS
MLR_all_WLS_car <- MLR_all
MLR_all_WLS_car$residuals <- residuals(reg_career)
var_func <- lm(residuals^2 ~ career + X3TGPAACAD, data = MLR_all_WLS_car)

# Obtain weights
MLR_all_WLS_car <- MLR_all_WLS_car |>
mutate(w1 = 1/abs(var_func$fitted))

# First WLS Model
reg_career_wls_car <- lm(X5GPAALL ~ career + X3TGPAACAD, weights = w1, data = MLR_all_WLS_car)

# Iterate
MLR_all_WLS_car$residuals2 <- residuals(reg_career_wls_car)
var_func2 <- lm(residuals2^2 ~ career + X3TGPAACAD, data = MLR_all_WLS_car)

# Obtain weights
MLR_all_WLS_car <- MLR_all_WLS_car |>
mutate(w2 = 1/abs(var_func2$fitted))

# First WLS Model
reg_career_wls_car2 <- lm(X5GPAALL ~ career + X3TGPAACAD, weights = w2, data = MLR_all_WLS_car)

residuals <- rstandard(reg_career_wls_car2)
predicted <- predict(reg_career_wls_car2)

# # Plot residuals vs predicted values
# plot(predicted, residuals,
#   pch = 19, col = "blue", xlab = "Predicted Values (Y-Hat)", ylab = "Residuals",
#   main = "Residuals vs Predicted Values"
# )
# abline(h = 0, col = "red") # Add a horizontal line at y = 0 for reference

```



## Implications & Limitations

- Non-constant variance could not be addressed through transformation and other remedies given this set of predictors

- Perhaps other predictors not captured by the High School Longitudinal Study are more reliably related to College GPA and choice of future career 

- Additionally, other regression approaches, such as Quantile regression, may be useful for exploring the significance of this relationship









